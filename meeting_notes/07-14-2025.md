## Project Information

**Project**: Multimodal 

**Date**: 07/09/2025

**Attendees**: @Ricardo Monti, @Haoli Yin, @Rishabh Adiga, @Sid Joshi 

---

## Updates Since Last Time

*Write naturally about what you‚Äôve been working on. No need for bullet points if you don‚Äôt want them - just share your progress like you‚Äôre talking to the team.*

**Ricardo**: Generally onboarding onto workflow ‚Äî really nice work üëè

Working with Rishabh to run semdedup using SigLIP2 embeddings. We ball-parked some hyper-parameters but ended up de-duplicating ~60% of data (aiming for 10%). Iterating on adjusting that.

Have also been doing some exploratory analysis of Mammoth data.. I‚Äôve seen a small number of examples of incorrect text and a larger amount of text that feels ‚Äúlow quality‚Äù

- quite a bit of this is chart related where the prompt/question is not related to the chart e.g., question is `Generate underlying data table of the chart.` or `Evaluate the suitability of the chart's visual representation for effectively communicating the algorithm accuracy data. Suggest potential improvements or alternative visualization methods.`
- will put together a list of `document_ids` which seem low-quality/problematic so that we can use these to sanity check any filtering strategies (e.g., perplexity filtering or similar).

Running experiments with varying learning rate schedulers to see if this is something work adjusting. 

@Haoli Yin 

- Merged in corrected data preprocessing and packing during training
    - done training end to end, waiting for eval
- Merged in better observability and WandB logging
- Got custom model inference working on vLLM
    - need to clean up that PR up, merge, and port over to run evals
- copied data to `datology-research-west1`
    - waiting for run there to observe training throughput

@Rishabh Adiga :

Implemented a comprehensive evaluation suite for our VLM models and initiated data curation work for the Mammoth dataset:

**New¬†Evaluation Datasets Added:**

- **CountBench**¬†- Object counting in¬†images
- **DocVQA**¬†- Document visual question answering with ANLS metric
- **ChartQA**¬†- Chart and graph comprehension
- **RealWorldQA**¬†- Real-world visual reasoning
- **Pixmo Points Eval**¬†- Spatial grounding and object identification

Evals cached to fsx and can be quickly used for signals without needing to load from huggingface.

- Launched semantic deduplication job using¬†SigLIP2 embeddings for Mammoth dataset. Performed one run aiming to filter out 10% of the data but resulted in filtering 60% due to too many clusters. Retrying this with different params
- Some bug fixes with respect to bos token.

@Sid Joshi: 

- Evaluation / Checkpointing Bug Fixes
- Perplexity Filtering Locally Implemented & Tested ‚Üí Will be running on Hyperpod:
    - Won‚Äôt be able to reuse the standard perplexity filtering flyte job due to a lot of custom logic to handle image tokens
- Rephrasing tested locally ‚Üí Running on hyperpod for now (depending on success for curation, will incorporate into Flyte)

---

## Discussion Points

*Paste your meeting notes from Granola, Otter, or whatever you use. Keep the natural flow of conversation.*

- transcript:
    
    Me: I don't know, like, people that are, like, more in the weeds with the evals. Do you have like, thoughts about, like, which are the ones that we can most easily target. Or, like, which are the ones where we might most easily see an improvement. Like this sentence up this sentence, the increased performance complex, semicolon, Datology sentence is what I'm talking about. I know this is a bit of a vague ask, because you know, we still are days, but like I said, it stands out. If not, think yeah. Yeah. I I think, like, as I've I've mentioned, like, some they're not, like, more like, PQA based on some generic images is something we should target. Right? But it's mostly Takes more Because those are not something that, like, bandwidth has in its data. Right. So, like, just adding the update of those will definitely result in. Large increments. Simple task list. We just have To the format in train. But, yeah, I'll also increase the token budget, but I guess since we we can perform same d loop and then add it, it should be fine. Yep. Yep. That make sense? Yeah. Yeah. Okay. So so I guess it's just the the eval is there already look fine with respect to what we can improve most on. Yes. I guess the the Okay. I'll remove MMU. Because I didn't last night, I covers. The Quick question there. How different is multi model? I know when you from default MMU? Because our our, like, actual text curation that you know, we're giving to other customers improves our MMU quite a bit. Do you think we could use that? It's so I actually have some categories other than words. I my Like So isn't that that good? Like, like, because doesn't have data that might be specific for MMA, like, like in terms of the knowledge stuff. If we could just simply find some data sets that are that are curated to improve and then and then new benchmark, then we could just use those. Right? So so and, like, science, math, art, history. Yep. It has many categories. Yeah. Yeah. Yeah. Each of those are those. Yeah. Yeah. Questions. So we need a lot of data. You need to Have Okay. Yeah. I I I think I agree. We can we can worry about that after the POC, I think. I think, yeah, like, Haoli said you need, like, we wanna, like, underpromise over deliver. So let's remove it. And then if we up crushing it, I think think this will be somewhere where we can probably learn something or, like, even plug in the already PCL curated data? Because that improves on MMOU a lot. Like, they for example, they they they basically mine on NMOU validation sets So to your point, say they find essentially the most similar examples put them in, it helps a lot. Okay. Alright. That's Cool. Okay. Cool. Yeah. Okay. Well, then this is just exactly what we need. I think that kinda system solves it. Cool. I can just remove I mean Yeah. Perfect. And then I guess, let's just with Rpmonti, you ping Ari or, like, ping the internal perceptron chat, and then we can move on from there. Yep. Alright. Back to this. Next point. Let's see. What should we discuss next? I think I easy to discuss thing is just regular release intervals. I think we have a set of PRs. Just, like, literally anything blocking is considered for the night next release. So currently, it's just the adding you finishing up the evals, which which we we basically finished. And then adding. Just fixing, loading the models, checkpoints, then once we're done with that, that should that should be done. The ticket where we're talking about, like, currently, when we evaluate loads all checkpoints all at once. Oh, actually, I I was doing some work on that yesterday, and like, what exactly is the problem? Because when I run the eval Mhmm. Like, if I run the eval, then it it submit it. Yeah. Yeah. Yeah. Yeah. Because you're evaluating the pretrained, and then the the fine tune, right, separately. Wait. No. I thought it's submitting the two checkpoints as two separate jobs. There's it's not two set. No. Not two checkpoints. Well, there are two checkpoints, but one one's for pretraining, one's for fine tuning. Right? Okay. But we we must I just We still train LAVA and then Okay. And then mammoth. Okay. That maybe But the problem is now with mammoth, we're what you've we train up to 10 epochs, and each epoch has a check point. So then there's, like, 10 checkpoints that that are being downloaded all at once. So so that that that's that's where the real problem. Yeah. Once I I guess I mean, if that's taking you too long or I mean, I've already finished I basically finished up the BLM. Maybe since you know more about the evals part, you can you can work on integrating BLM because I have I have the model already loaded into VLM. Okay. So it's now just replacing the the the inference with VLM now. Okay. So I I can sync with you, and we we can hand off, and I can work on that checkpointing thing. Okay. Let's do that. What exactly is the issue? Are you referring to the eval only for Not just evals only. Just oh, yeah. It would Because when you're launching evals, when you look at the logs, it's actually, like, you for for mammoth, for example, if you train up to 10 epochs, there's 10 checkpoints. Right? Mhmm. So then each checkpoint is, like, 12 gigs, So you're, like, downloading a 100 over a 100 gigs of checkpoints. Into FSx at once. And that's not ideal. You should be, like, doing that one at a time based on, like, yeah. Basically, just one at a time so you don't overload FSX. Oh, I think that's that's a simple fix to the launch because right now, this launching panel job. Like, I was seeing that hey. There were two jobs launched when I when you have two checkers selected as you wanna evaluate them. Okay. But it was just loading it. Two checkpoints in each job. There wasn't, like, two separate It wasn't, like, retrain and find you. Oh, really? Yeah. So it it just, like, it it's been a temp slur job, like, and that will and it will do it one by one. Because it's separate jobs. Right? Yeah. I think I think it's generally okay to to evaluate in parallel. I think we just need to cap the amount of parallelism that we're having. Like, so So can you split assertion? It's the x script, like, stated or not? Okay. Yeah. Okay. Then then then then don't worry about the eval part. I I can keep bringing on the BLM stuff. Okay. And then next Maybe we can just quickly the dogs you are talking about in a couple it to where to say that Sure. Sure. Yeah. About different things. Okay. Sure. Yeah. Yeah. Yeah. We can do that offline. One sec. Okay. And then next, I think I'll I'll leave that last. So previously, I talked with Matthew about some stuff. He has some pretty good ideas, actually. So talking about the perplexity filter, since we're going to be training domain specific models anyway, I think it could be interesting to use that as a perplexity filter. For different domain specific data because domain perplexity is different from like general perplexity. Mhmm. And know that that could be more informative if we want to, like, sample more from a certain domain. Mhmm. So that's something to think about once we have that. And and then after that, I think another thing is I'm trying to remember what he exactly said over here. But synthetic generation, I think this is kind of along what Sjidar said. About synthetic data. It's like divide coarse grain versus fine grain. Fine grain versus meaning, I think what Matthew said was, a new capability. Versus, like, what is more generally, like, similar evals. So so some sort of, like, clustering or, like, prior that we have to Yeah. This is exactly what I was saying. Right. Right. It's easy to improve on UAS that are more specific. Right. Right. Exactly. Yeah. Yeah. So I I think I think gearing, like, having an idea of what the that for cure for synthetic generation I think yeah. Yeah. That was just some high level points Just wanted to let you guys know. And then I guess this kind of for goes into the data mixing plan. So I was, like, thinking a little bit more carefully about this research problem. It's it's pretty hard. There's, like, basically no literature about it. Even in LLM land, it's, the the concept of using model merging as a for assessing data mixtures is very a very new concept for some reason. Mhmm. Or either that or it's just not been published very much because it's been successful. Mhmm. So what so I guess from our side, it's like we start with a lot of checkpoint, We can train for a fixed amount of compute depending on how whatever the the largest domain is. We just set that as the compute budget. Mhmm. And then and then we end up with eight checkpoints, and then we have 10 evals. Right? Mhmm. So if you think about the power set, it's two to the eighth, which is 256 like, combinations. Across the eight checkpoints, like like, just linearly merging, not weighted or anything. Are you gonna do that null souping type stuff? Yeah. Yeah. So so so we we can I mean, that's still a lot of eval? So I was thinking about, like, search strategies and stuff like that, but 256 checkpoints is technically feasible. It's not, like, nontrivial, but it's, like, is doable as, like, a baseline. Mhmm. So I was thinking about just, like, running all of it. To eventually find, like, a better search algorithm later on. When we do have, like, many more DSS and domains. Was one one of the first steps I was thinking about. Assuming it should work because we're starting from the same log Yeah. Technically, yeah. Because we're not changing we're not changing that. Like, if you if you think about it, the vision encoder encoders fixed. Like, it's frozen. Okay. The only thing that's changing the most is probably the projector. Okay. So, like, where it should be, like, like, the optimization landscape should be, like, linearly inter interoperable. I see. So there is the the problem is just, like, when you do go beyond eight domains, you don't wanna do this power set anymore. And there's also no reason to stop it too. Right? Yeah. And You could sue Yeah. Well, the power set does soup, like, all of them eventually. I see. Sorry. Two q six is because the binary is on and off for each Yeah. Yeah. Exactly. Yeah. I understand. Sorry. Yeah. Okay. Yeah. So so there's that So I think I was, like, chatting with Claude, and it was, like, talking about you could do some sort of, like, beam search over all the combinations. Like, start with all of them separately and then start combining and and using, like, like, this kind of beam search to find find the scores. But but that depends on but then you have to think about how each eval is unique Yep. And thus, you would have to do this over all evals in order to optimize for certain eval. Mhmm. So I think just doing it naively for 256 because you would be able to calculate all evals at once. That's something that we can do, but then I guess yeah. I just wanted to get you you guys thinking. I'm like, because I think this is just, a new research field. Yeah. Well, model snooping is is interesting. The the other challenge is that, like, you might not wanna, like, weigh them uniformly. Right? Because that that that allowed another dentist to pair. Right. Right. Right. Do that, but, like yeah. Yeah. And I think people use more, like, for that, they use other algorithms, like they model whole thing as, like, a distribution. And Right. Right. Yeah. You do Bayesian methods on top of that. But Right. Right. Well, wasn't the plan to do the waiting? Like, initially? Well, we're not we're not gonna wait right now. We're not now it's just, have an indicator function of including it or not. For simplicity's sake. But I don't know. That's just where where I'm probably gonna start. Okay. Once once we get the v one off, and see where we can go with that. But, yeah, if you guys can find any art like literature around that, that'd be great. Okay. And then I guess just things for next time. Think we just about know what we're gonna work on this week. Unless people have comments about it. But I guess Oh, sorry. I think one one point that we still need to talk about is, like, what does our v one of curation look like? Mhmm. Mhmm. Because I think I think that's the main point that, like, the execs call care about is, like, okay. We have our baseline now assuming that we we we get that shipped soon. Yep. What does our first round of curation look like? It's like, obviously, it'll probably have some some very limited time you do upon it. Yeah. How close are we do we think I think so, like, thinking about what works for sleep and what works for, like, text in general. And, like, low hanging fruit. Semi Juice is, like, low hanging fruit, I think. Some version of mining could be low hanging fruit if we can embed the images for the e files. If that's not much work, then then we can mine immediately and and see Synthetic rephrasing, I think, will definitely help a lot, but, like, traditional, like, that up and running, I guess. Mhmm. And then, yeah, sometimes kind of, like, filtering. Mhmm. Yep. Yeah. We need it on the embeddings for Correct. That's what we that's what yes. Yeah. We don't have it here. Here. Yeah. Yeah. That's right. Right. Yeah. I think we could so we Got it. Together. Yeah. Yeah. Okay. That makes sense to me. V one curation is just to get some signal that things are improving. Right? Not like Mhmm. That's correct. Just to give give us internally some confidence that we should do a POC. Cool. Cool. And and, hopefully, we we should probably try to sprint towards it soon. Yes. I guess I guess do do we have a timeline review on curation? I'll make Like, just getting all these things and then getting a run through with them? Yeah. Let's let's maybe try to let's let's I just sent it to before close. Much done because we just we just had some issue with it. Like, 60% instead of 10% even though we gave Got it. Yeah. We know the reason, but we should have it the the ticket is valid for the next. Yeah. We have to work on the export because the export the scheme is a little different. So that that we can run and get that going. With. With. Yeah. Because they said you you trouble do WXIX four per clip. No. But we don't need WXI They do. But I think we're close. We just need to, like, do the the export once it's done. And what we basically see. So any any, like, insights on text rephrasing, whether we should include that? Yeah. So I I have it working, like, I I have, like, a tracker working with computer and metropod, but I I I I think I think it's generally fine. Like, right now, I think we're still in the proof of concept for text. The general workflow is don't put things into flight until you know that they work. Right. Or So I think it's fine as it is. Okay. Then then I can just run it like your going Like, I have the problem Yeah. And then I guess another thing that we do is that every time you make a curation algorithm, want to run it in isolation to make sure it works. As in, like, an ablation. And and then we'll we'll work on composition later depending in see how we adjust with these knobs and, like like, print hyper banners and stuff. Let me know and see if I can help at all with, like, scaling that up. How are you burning it at the moment? Requires a lot of lines. Cool. I was wondering, like, for the days, but I don't, like, to be, like, this to see that the numbers reasonable. For that. I mean yeah. What was the question? For the baseline, like, I know we like, I I further run because probably before we, like, push up the Yeah. Yeah. Yeah. So so so it it did I mean, it did crash because it the reservation ended at 90%. And so so we we do have nine epic of training, which is, like, well over So, I mean, we can also eval over all nine checkpoints to see how it, like, progresses throughout training. So so, yeah, I can hand that off to you, and then you can run the Yeah. Yeah. And then we should have signal by that for, like depending on cost or usage. To HMRO. So Just want you to just follow-up in the eval eval Right? Yeah. I can give you the config initial. Should just retrieve it automatically. Yeah. You have to create a config where you gotta only swap the check. Yeah. Oh, sure. Yeah. Saying he's he's on the Yeah. Yeah. Okay. I got that to you. I know. Yeah. Yeah. We we should have that baseline soon. So I guess, like, we're we're I mean, it wasn't run with, like, the observability stuff or, like, you know, end to end with everything. We'll do that later. But I think I think we have we have, like, a base baseline run technically. Because It's a preprocessing. Yeah. That was the one with the preprocessing in it. Yeah. So that's the that's the only blocker for that one. Yeah. Yeah. I think we're basically done. Yeah. I mean, as far as I'm aware, there's nothing, like, causing we could you have to check. To see if that you have them put it in with Yeah. That's right. Alright. Cool. Then we cook. Alright. See you in next meeting, Yeah. Bye.
    

Discussion: 

- Addressing Ari‚Äôs comments on Perceptron POC

[Perceptron POC](https://www.notion.so/Perceptron-POC-222575b6d708807ebf61ca66778d2c4b?pvs=21) 

- Data mixing research plan
    - start with Llava ckpt
    - train each mammoth domain for fixed amount of compute (whatever the long
- What Haoli previously discussed with Matthew:

![image.png](attachment:93b9f86b-22ec-4005-ab86-31f8939f46a3:image.png)

---

## Immediate TODOs

*Tag people naturally for tasks that came up during the meeting. Write it like you‚Äôre sending a Slack message.*

- Use semdeduped data to perform¬†data curation ablations for training

@Rishabh Adiga 

- create embeddings for evals for mining

@Haoli Yin : 

- verify eval logs with @Rishabh Adiga

@Sid Joshi 

- Perplexity Filtering: Run on 12M data
    - Deliverable: Training runs on various slices of data sorted by perplexity
- Rephrasing Text-Part of the Data: Run on 12M data
    - Deliverable: Train on rephrased data

## Notes for Next Time

---

v1 curation: basic signal if our curation works

- SemDeDup (basically done)
    - need to fix WDS export job
- Text rephrasing
    - have working at 50k samples but need to run over all 12M and run ablation
- Perplexity filtering (version of DFN)
    - implemented, still needs ablation
- some version of mining bc we have SigLIP2 embeddings
    - working on ingesting and embedding evals

v2 curation: aiming to hillclimb evals

- MM-GEN (requires more cycles to get it working), getting mining working is kind of a dependency
- Data Mixing

*Template tip: Write naturally! This gets parsed into Linear tickets automatically, so just focus on capturing the conversation and action items clearly.*
