## Project Information

**Project**: Multimodal Team

**Date**: July 16, 2025

**Attendees**: @Rishabh Adiga, @Ricardo Monti, @Sid Joshi, @Haoli Yin 

---

## Updates Since Last Time

*Write naturally about what youâ€™ve been working on. No need for bullet points if you donâ€™t want them - just share your progress like youâ€™re talking to the team.*

Team effort: 

- fully debugged evals, identified pain points and in progress of addressing them

- @Ricardo Monti :
    - Wrote a perplexity filtering and wds output job for Flyte: https://github.com/datologyai/universe/pull/2590
        - after some (slow) debugging, it appears to be working (sanity checking [dbrx notebook](https://dbc-946cdf37-a22a.cloud.databricks.com/editor/notebooks/752167986897954?o=7483310983345844#command/5061010742831955)).
        - helped Sid onboard and launch first flyte job ðŸš€
    - next steps:
        - going to run some Kmeans style experiments mammoth data â†’ initial shows showing clusters relating to charts, graphs, etc
        - having clusters will help with:
            - being able to up/down sample experiments during training (eg if targetting particular evals)
            - mining from datacomp to get more image diversity
        - going to do some light EDA over mammoth text and possibly apply some filters (had see some examples of poorly formatted text in some of Sidâ€™s EDA explorations for PPX).

@Rishabh Adiga 

- Performed semdedup with siglip2 embeddings on the Mammoth dataset but we saw that job was removing 60% of the data even though the target was set to 10%.
- We initially thought this was because siglip2 embeddings are too tightly packed in the embedding space.
- To verify if this is the reason, ran the same job with dinov2 embeddings but ran into the same issue.
- The reason is that mammoth has a lot of exact duplicate images. Ran the exact dedup job and found that 7.6M images have atleast 1 exact duplicate.
    - This is reduced 7.6M to 1.6M samples (but we think doing this is not the best idea)
    - distribution of the number of duplicates across these 1.6M samples
        
        ![image.png](attachment:aa80b999-49f4-40c4-93a1-29411652f4b9:image.png)
        
- Other than this, worked on fixing some bugs with image exactdedup(used to filter out all of the textonly data because they had the same hash) and semdedup jobs (did not accept text only data) and helped with some preprocessing issues.

  

@Sid Joshi 

1. Evaluation pre-processing bugs resolved; post-processing made more lenient where appropriate â†’ landed our first correct baseline results on all perceptron benchmarks!
2. Perplexity filtered datasets being written out! (thanks to @Ricardo Monti!)
3. Merging the text of examples with same image PR in progress (target: EOD July 16, 2025)
4. Rephrasing will be run on top of this to maximize net token efficiency (PR ready, waiting for merging to launch)

Next Steps:  Complete 3 & 4 and train â†’ eval loop with 1) ppl filtered subsets 2) merged text data 3) merged + rephrased text data 

@Haoli Yin 

- Got WSD cyclic schedule working and will merge in soon
    - end to end run in progress, will need to eval all checkpoints afterwards

---

## Discussion Points

*Paste your meeting notes from Granola, Otter, or whatever you use. Keep the natural flow of conversation.*

- How do we begin scaling experiments?
    - what data do we need?
    - What experimental setting should we adopt?
- Fun topic: VLM reasoning (Vikâ€™s talk)
    - good synthetic data is protected info (clock scenario)
        - they used blender to create more synth data
        - if thereâ€™s easy programatic pipelines but important then we will want to invest effort here
    - geoguessr sample efficiency
    - counting >3 requires grounding/pointing reasoning traces

---

## Immediate TODOs

*Tag people naturally for tasks that came up during the meeting. Write it like youâ€™re sending a Slack message.*

@Rishabh Adiga 

- take 3.5M unique samples from output of SemDedDup to do EDA on them
- running training on the exact deduped data
- run ablation with and without text samples
    - take a look at what evals adding text data actually improves
    - InternVL3 found that 1:3 text to multimodal data was the optimal mix

@Ricardo Monti 

- onboarding to datopenqwen
- help with perplexity filters, semdedup workflows

@Sid Joshi 

- mixing deciles of perplexity to find optimal range
- running rephrasing training

@Haoli Yin 

- will run domain-specific training on our v1 infra
    - will run evals later with eval-only configs
- eval work:
    - make evals configurable - QoL
    - figure out how to increase batch inference from bs=1
    - work on vllm integration after kicking that off

`

---

## Notes for Next Time

*Anything we should remember for the next meeting*

- Leave adaptive image tokenization for the backburner
    - maybe prioritize more if we care about how data curation influences image tokenization

- Target v1 data curation on just mammoth to figure out if our curation leads to significant gains
- Begin downloading Mint-1T in parallel for later usage, talk to Bogdan
- split into two initiatives:
    - modifying to support native resolution so we donâ€™t blow up token counts when doing interleaved multi-image training
        - maybe test with the mammoth subset of interleaved images/video since that already exists
        - Goal: support native resolution NaFlexViT that SigLIP2 version uses
    - Study the Mint-1T format and how we can get a baseline run working with our existing 729 images tokens even if it blows up training to figure out where the bottlenecks are

---

*Template tip: Write naturally! This gets parsed into Linear tickets automatically, so just focus on capturing the conversation and action items clearly.*
