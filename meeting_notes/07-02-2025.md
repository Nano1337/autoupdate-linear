
## Project Information

**Project**: Multimodal Team

**Date**: 07/02/2025

**Attendees**: @Haoli Yin, @Sid Joshi, @Rishabh Adiga

---

## Updates Since Last Time

@Rishabh Adiga

### **SigLIP2¬†Embedder Flyte job created**

Much of last week was spent onboarding onto flyte and understanding how it works. Special thanks to Parth for all his help on debugging flyte issues! üëè

Implemented SigLIP2 model support (`hf-hub:timm/ViT-SO400M-14-SigLIP2-378`) as an embedding job.

**Initial Approach: Spark Implementation of the job**

- But spark jobs¬†don't have GPU access

**Solution: Ray-Based Implementation**

- Redesigned as Ray-compatible embedders (`SigLIP2ImageEmbedder`,¬†`SigLIP2TextEmbedder`)
- Integrated with existing embedding¬†infrastructure

The SigLIP2¬†embedder was implemented to support¬†two critical data processing workflows:

1. **Semantic¬†Deduplication**
    
2. **Synthetic Data Generation with MMGen**
    

### OpenCLIP vision backbone model training implemented

As discussed in the [CLIP and VLM Eval Correlation doc](https://www.notion.so/CLIP-and-VLM-evals-correlation-analysis-21d575b6d708807fa944c86da7072294?pvs=21), we wanted to see whether our retrieval optimized CLIP model performs on par with Siglip models of the same scale.

![[Pasted image 20250702133915.png]]
**Why retr-opt performs¬†poorly on VLM evals:**

**1. Training¬†Objective Mismatch:**

- **retr-opt model**: Optimized for retrieval tasks¬†without text decoder loss/sigmoid loss
- **VLM evaluations**: Require strong text generation and reasoning capabilities

**2. Resolution Limitation:**

- **retr-opt model**: 224px input¬†resolution
- **SigLIP**:¬†256px input resolution
- Higher¬†resolution provides more visual detail¬†crucial for tasks like AI2D diagrams

@Sid Joshi,

- Code clean up
- Working on text-only rephrasing:
    - Works well with strong model like Claude: token count cut by 50%, without losing quality of text (qualitatively)
        
    - Working now on reproducing this behavior with a small local model
        
- Blocked on MM-GEN implementation due to embeddings not being available
- Running longer training of 2-stage training (phase 1 - adp only on llava 558K, phase 2 - multi-epoch on Mammoth)
![[Pasted image 20250702133947.png]]
![[Pasted image 20250702134001.png]]

- Statistics and Details:
    
    ```
    === ORIGINAL TOKENS ===
    
    Mean: 172.3
    
    Median: 164.0
    
    Min: 13, Max: 494
    
    Std Dev: 117.5
    
    === REPHRASED TOKENS ===
    
    Mean: 86.1
    
    Median: 87.0
    
    Min: 11, Max: 192
    
    Std Dev: 50.3
    
    === TOKEN DIFFERENCES (Rephrased - Original) ===
    
    Mean difference: -86.2
    
    Median difference: -63.0
    
    === TOKEN RATIOS (Rephrased
    
    Original) ===
    
    Mean ratio: 0.58
    
    Median ratio: 0.58Longer after rephrasing: 0 (0.0%)
    
    Shorter after rephrasing: 17 (100.0%)
    
    Same length: 0 (0.0%)
    
    === EXAMPLES ===
    
    **Example 1:**
    
    Original (132 tokens):
    
    HUMAN: <image>
    
    You are given a screenshot showing a webpage with a red bounding box. Perform OCR to capture the text within the red bounding box.
    
    GPT: The coordination of metals by nonmetallic molecules is important for many applications, such as chemical separations and electrolyte design. We therefore collect information on the binding properties of metals in molecules. These properties, especially thermodynamic quantities like binding energy, can be thought of in terms of the general reaction A-M ‚Üí A + M, where M is a metal and A is some molecule. The process of calculating metal binding properties requires additional information about molecular thermodynamics, bonding, atomic partial charges, and atomic partial spins.¬†¬†¬†Rephrased (76 tokens):
    
    HUMAN: <image>
    
    Perform OCR on the red bounding box in this webpage screenshot.
    
    GPT: Metal coordination by nonmetallic molecules is crucial for chemical separations and electrolyte design. We collect metal binding property data, especially thermodynamic quantities like binding energy, represented by A-M ‚Üí A + M reactions. Calculating these properties requires molecular thermodynamics, bonding, atomic charges, and spin information.
    
    **Example 2:**
    
    HUMAN: <image>
    
    Evaluate the potential impact of climate change on freshwater withdrawal patterns for different sectors, and discuss strategies for adapting to these changes.
    
    GPT: Climate change is likely to alter freshwater withdrawal patterns for different sectors in several ways. Rising temperatures and changing precipitation patterns may increase water demand for agriculture, while also affecting crop yields and water requirements. Domestic and industrial water usage may also be impacted by climate-driven changes in population growth, urbanization, and economic development. To adapt to these changes, strategies such as water storage infrastructure development, water-saving technologies, and climate-resilient agricultural practices must be employed. Additionally, water resource planners should consider scenario-based planning approaches to anticipate and prepare for potential climate-related disruptions to freshwater withdrawal patterns. By integrating climate change considerations into water management practices, we can reduce the vulnerability of water resources to climate-related shocks.¬†¬†¬†Rephrased (87 tokens):
    
    HUMAN: <image>
    
    Evaluate climate change impact on freshwater withdrawal patterns across sectors and discuss adaptation strategies.
    
    GPT: Climate change will alter freshwater withdrawal patterns through rising temperatures and changing precipitation. Agriculture may face increased water demand while crop yields fluctuate. Domestic and industrial usage will be affected by population and economic changes. Adaptation strategies include water storage infrastructure, water-saving technologies, climate-resilient farming, and scenario-based planning to reduce vulnerability to climate-related disruptions.
    ```
    

@Haoli Yin:

- Set up HF model upload and inference scripts, model has bad behavior, likely because it was trained with only 144 image tokens instead of usual 729 and wasn‚Äôt trained well. TODO: need to test with better/correct ckpt
- Debugged model training preprocessing wds script to reach parity with that in `LargeFinetuneDataset`

---

## Discussion Points

_Paste your meeting notes from Granola, Otter, or whatever you use. Keep the natural flow of conversation._

- talk about what Matthew discussed
    
- Amazon POC
    
    - CLIP vision encoder improvements?
    - Try existing cls-opt and retr-opt on more modern CLIP evals - this needs to be determined based on what we want to propose for POC (discussion point)
    - Train CLIP with text decoder loss following CLIPS/OpenVision Paper
        - can we improve synth data here in VLM style but for training the CLIP model?
        - baseline: train with our retr-opt dataset with so400m model + text decoder loss
    - **Ask**: can just use the Perceptron POC to get started with the Amazon engagement instead
- Transcript:
    
    Me: Okay. Yeah. You guys you guys finish that up. There's I guess, two main discussion points today. One is I had a one on one with Matthew. He brought some interesting research points that we can talk about. With regards to, like, data mixing and other, like, improvements, like, thinking about how algorithms can complement each other So if you improve what algorithm improves along this axis, maybe you can find another algorithm that improves along another axis. So that so that they're they're mutual in terms of, like, attacking different aspects of curation. We immediately dig into that a little bit. And then I think Amazon still wants us to respond to the POC there. I think even though they they kind of listed unrealistic evals, the point is they just wanna work with us, and they just did use that as a starting point just just because they could. But it's up to us to, like, scope it out and to say exactly what we got on to improve on and think about what would be the most relevant possibly or what what would be the most attainable for us in the short term. Just to show that we we, like, we're we're legit, and we know what we're doing. I I mean, the Amazon deal is on the tip side. Mhmm. So, like, you already have a bunch of Well, no. Not yes. But the for the POC, like, depending on what evals we care about on that side, like like, obviously, they they're not gonna care about classification or, like, MS Coco. Right? Gonna care about more modern equivalents. So we're gonna take a look at those. And then in general, we do need to spin off the clip initiative. Which is, like, improving vision encoders for for VLMs. Okay. Because we need to because we have that basically. That's our guess, edge in house is that we can do that. Instead of just relying on Cyclip. Even though Cyclip two is is a really good model, there's definitely a complimentary curation that we can do to you know, the the the domain of VLM evals is very different from CLIP evals. Right? So thinking about how we can improve the ECLIPSTACK as well. Could allow for more improvements. So I guess, like, one. Simple experiment. Yeah. Okay. Let me come back to this, but that's just something for you guys to keep on your mind for now. Seems like it was a very productive couple of days So you wanna just kick us off, Sjob? Yeah. Sure. So, yeah, like, most of my week, I know I can figure it out, like, stuff. The only queued up as well for the mandatory but I think it's in suspended state. Yeah. But, yeah, I I I'm guessing I I think, like, on the twentieth example part here that I had, Mhmm. It looked like like like like eight minutes or something. Twenty yeah. Yeah. That makes sense. Yeah. So for, like, my my data is a hell of a be. Well, it depends on how many how many GPUs is there. I I I think it has ninety sixty two GPUs. Six sixty two? Yeah. For the 20 k party. I'm assuming it's gonna do something similar. It should ideally give me more if it's dynamic. Because, like, the size of the data Yeah. If you get the entire cluster then, then, yeah, it should be fast. But assuming you get that many then Yeah. So it I mean, I I think if we do get that, it it should like and my job starts, it should be done by the end day. Yeah. Yeah. Makes sense. Yeah. Okay. I mean, Yeah. So, like, why why you took like, like, why it looks a little longer than expected? Was I initially Yeah. And then I just was sure But now it's Huddl. And Yeah. Why why this is important? I guess this is more contextual. Context for others. It's just because we do 78. But you're right. I do like it over It will looks like it's performing for the year. We don't have. Oh, not really not really sigmoid loss. It's, like, the technique texting control. Yeah. Yeah. But but but I guess another factor is that they're also, like, in terms of the, like, the pixel size, it's, like, two twenty four plus two fifty eight, which I think impacts probably more resolution. Yeah. Yeah. Interesting. And I think the reason it's it's similar on that with same score on Advisor is because what more is that this bag? It's not that relatively good optimized model catches up to save the Yeah. I mean, it's math. I don't think we have much. Yeah. It's because Oh, wait. How do you train as well? It's just just with like the stage one open coin and in lava? Or what what Okay. It's the CI. So datacom Plus. So the fine tuning stage is the lower data set. See, now the pre trained Yeah. That's probably that's probably why you're seeing the reason results because it's because Yeah. I mean, the reason I did that was because all the other more different of prepared. Yeah. Okay. But but using that, I don't I don't think we can make any conclusions, though, because We just make the math. Aside. Or or, like, maybe even even in general because a lot a lot like, Mammoth is just a lot better Radiga said. Yeah. So so I I I don't know the whole I don't know. I'm guessing like like, everything's what I do. Okay? The understanding from this result is going Right. Right. Yeah. Know. Yeah. I know. I get it. Yeah. Yeah. I I'm just wondering because there might be more more of a noise floor we're dealing with here. Okay. That way. So it's hard to know what is significant. Yeah. Yeah. But, yeah, no, I agree. I I think I think generally, Sigleb is is better because it's trained for, like, more of BLM aligned tasks. Okay. Next up. Sid. Yeah. So yes, yeah, since last week, the bug fixes in there. That was I was like, since my day, I've switched to working on, like, the activation stuff. I stopped Urinary embeddings and I consider it will there's a lot of duplicative work. Yep. I'm gonna end up taking this as Makes sense. So I'm just waiting on that. Instead of to working on rephrasing the question answer part. Right? As we discussed, Updates part of that, like, seems we know that this strategy does work. Now the next step is getting it to work with our tool. Today. Get it. Get the local model to behave Also switched Giving it the prompt and stuff. Yeah. And yeah. Like, every other day, hope with this. Debugging Planning that later that add the code in the flight pipeline to run this on the flight It's Right. Like, something like that locally. How much, I guess, resource request So this is you're gonna be running rephrasing over what what exactly? So I want I'm trying to use a point three eight three model. Uh-huh. Approximately, let's say Okay. I guess, like how many samples? 12,000,000 samples. Like, of Yeah. All of them. Okay. Basically, like, I'm That's it. I mean, it it shouldn't shouldn't shouldn't be a problem then because ten ten million isn't, like, much data. Yeah. I'm thinking, like, since we're getting more cluster capacity, maybe we move some of that cluster to Alright. Move it on the maybe maybe the the east Cluster. Let's 32. Node one to EKS on Hyperpod. To generate synthetic data with that one. And maybe you can, like, I mean, just queue a job there as well. And that should be done very fast. Right. Yeah. Yeah. So the nice thing is once I figure out, like, what we need, I can reuse the synthetic text team. Code, basically, entirely just changing the Yeah. Exactly. So yeah. I'll just use where I mainly have I mean, like, I I I mean, like, yeah. Yes. Yeah. It's good. But it's just you might have to be a bit more proactive with asking for that. Because they're they're gonna be running stuff on their side and we have to make sure we have enough like you know, resource allocation Got it. Yeah. And also just to make sure that you're on, like like, when they get that cluster up, you're ready to go. Go. Yeah. Yeah. Cool. So so just testing and debugging all the flight stuff locally. For now. Yep. Cool. Cool. Yeah. So I saw earlier this week, it was just setting up the model on Huggy Face and doing inference I think I think it's not anything else. It's just that I I wrote the I I, like, literally stripped out the eval processing scripts and used that for inference and the model still still isn't is hosating a lot. It's literally the checkpoint that is the problem because it's only using a higher 44 tick tokens in. And it's not great. Yeah. So, like like, well, once you've training that model, I'll I'll you out that model. And then continue that work stream. And then right now, oh, yeah. And then also, debug, the prebubbing things with our script. Yeah. Yep. And then I guess, yeah, we'll come back to discussion a bit. But immediate to dos is I also like, just told Cloud to to productionize our logging. So it I'm currently testing that on a single GPU to make sure it works. There's some little bugs, but I'll get through those. But it's gonna introduce a lot better logging overall for, like, like, even gradient norms Yeah. Like, you know, cluster health, stuff like that. So it's better for observability. And then, of course, we're gonna be able on inference after I get the in what? I mean, yeah, I can show straight up show you what it is doing. So, like, yeah, you guys can probably I'll just I'll just add you guys to this. I'm still testing it. There's some bugs, but but so there's there's two over here. Mean, if you wanna go through the full features, it like writes it all here. You just read it? But, basically, it just makes a metrics metrics class that that is like a state that tracks the state of the and and and, like, manages all the wannabe stuff. So it just kinda centralizes the that. Oh, usually. Yes. Yeah. Yeah. Oh. I just need to verify that everything actually works. Yeah. Because it already has you. Trust the on these utilization that results. Right. Right. Right. Right. I have a another request on this, like, Yeah. We could add like, every key or some steps that would be nice. Like, we can take a small sample doing. Until I eval, and, like, I can't get Yeah. I think for the eval while train initiative, that would have to be, like, a separate thing. This is just logging basic, like, during training. But I think in the validation part, we have to think about more carefully about what you want what you want for the validation side because you don't want and also how you would integrate that because you don't wanna pause model training to run validation. Because that's like So I was thinking in the simplest case, we just we just circle. Like, we have a small validation data. in like, the same training. Uh-huh. And just sample the validation batch, compute the loss Like, not doing properly that because like, actually doing the eval something No. No. I I I understand what you're I understand what you're doing what you're saying. Like, so that when when we doing clip last summer, the setting things up, it was trained and then eval saw a small subset and then trained eval, but, like, the the eval time ate into the actual, like, cluster utilization. Right. So and and you you don't want and then so it's much easier to train, so you don't wanna deal with this. Or you'd still do some sort of, like, clever pipelining to make sure you train and then you know, happen somewhere else. At the same time. Like so that I think we need think a little bit more carefully about this. Because, like, yeah, yes, we can naively just put in, like, like, a small validation in between, like, during checkpointing, like, yes, we can do that. But what are the trade offs? You know? Yeah. They I get your point. I guess I'm thinking something that's, like, some way. I like, I'm not sure if this is, like, yeah, I think definitely makes sense to take care of the body. Something in the next couple of days that we can add, like, No. No. I agree. I think I think it should it should be fine. I guess within Just like a handout. I'll say it on the Yeah. Yeah. I yeah. I think this also kinda gets into, like, infra a little bit. Yeah. It's like well, in order to do this, you also have to reload the checkpoint again. So, or I guess, could you persist Like Just do it via like, so in the training room, just be like, if x Oh, I see. I see what you mean. Yeah. I I I mean I mean, you just you just change it to eval mode. Like, it's not I don't think it's not The main idea is managing the data loader. Validation method. Like this. Oh, yeah. Yeah. Yeah. Like, what we could do for some Yeah. And, like your phone your phone. Yeah. Yeah. So, yeah, that makes sense to me. And okay. Yeah. No. That's actually pretty straightforward. Yeah. I guess the question is, then we just need to figure out what what what a validation set looks like. We we we can stop it. An amicepam, and then as soon as I finish this, I think, Okay. Cool. Cool. Yeah. And, also, probably get some more other info stuff. It's just supporting preemption and resumability. Because that's important. In case our run fails as well. Oh, there is that. Defeated. Oh, really? Nice. Interesting. That the observant is than that. Yeah. Yeah. We can see see where where the well, I mean, hopefully, we can take it for a while. Be fine. Yeah. Yeah. Hopefully, we'll have more more hands on on on infra stuff later on too. Because I I don't wanna really deal with this stuff. Anyways, let let us quickly finish this. So I guess discussing immediate to dos for now. Rajab. Yeah. So should is mommy's stuff to sit onto d two. I mean, that's stuff. Got it. Makes sense. As I said just okay. You're still on the rephrasing stuff. And yeah. That makes sense. Of Oh, yeah. And probably the next one is trading. The three phase Oh, Okay. Cool. Quickly on the discussion point. I think we got, like, a minute. This is lower priority. We can talk about that later. About I mean, honestly, it just train with a text decoder loss and see what happens. Use use our retrieval datasets. If it improves, and then plug it into the VLM. That's like a baseline. Yeah. No. They they said the point why they wanted to propose this first is why the eval's look like VLM task is because they're waiting us for us to get our VLM stack going. Right. So, like, yes, we can. It's just I mean, it it is is that what what do you think that's easier? I I do feel it it'll like, in case there is some forklift duration stuff that you guys needed to, like, apart from just repurposing the existing I think I think we just had ask Ari to ask them because their original request for BLM was much wider scope. Like, a lot a lot larger, like, 500 beats of trading. Yeah. Right. So, like, Yeah. Like, we they've gone through, like, what they wanted, like, something. Like, lately, like, Like, something. We just want the bare minimum. So Okay. Yeah. I I agree with that. Because then we worked in this one thing and that way. Okay. Cool. That was good to love everything.
    

---

## Immediate TODOs

_Tag people naturally for tasks that came up during the meeting. Write it like you‚Äôre sending a Slack message._

@Haoli Yin:

- improve WandB logging: implemented, currently testing on single GPU, need to test in multi-node setting
- set up vllm inference for evals

@Rishabh Adiga

- Going to be working on improving evals in our vlm training pipeline to help track validity of the scoring. Modify pipeline to track the following:
    1. Raw model output
    2. Parsed model output (this is already saved)
    3. parsing_success: was parsing successful or not? This will look different for each parsing scheme, may not be well-defined sometimes too, take your call with how to surface this
    4. "correctness": already there
- Semantic deduplication once flyte job for siglip2 embeddings on Mammoth 12M finishes running

@Sid Joshi:

- Finish iterating on prompt for rephrasing and run on sample from mammoth.
    - Deliverable: token-efficiency numbers & plots
- training with the rephrased data

---

## Notes for Next Time

_Anything we should remember for the next meeting_

- Think about how to write in a validation loss calculation during checkpointing or every n steps
    - maybe 10k samples from mammoth
    - since this is small maybe just fetch directly from fsx to not mess with the wds train cache
    - this could go hand in hand with the MM-GEN work since that requires a validation set as well
- Other infra stuff:
    - support pre-emption properly
        - resuming training

---

_Template tip: Write naturally! This gets parsed into Linear tickets automatically, so just focus on capturing the conversation and action items clearly._